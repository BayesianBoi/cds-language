{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHLDxJdRzBi"
      },
      "source": [
        "# Session 9b - Dynamic topic modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3VGFZ1USMTu"
      },
      "source": [
        "# Data\n",
        "This time, we're going to load an work with an English language novel. The goal of *dynamic topic modelling* is to study how topics evolve and interact over time.\n",
        "\n",
        "In this case, we want to know how different topics evolve and change over the course of a single novel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJij3WP6SEQD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"path/to/novel\", \"r\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're first going to do some *very* crude pre-processing by splitting the text up into sentences based on the presence of period/full stop.\n",
        "\n",
        "**Question:** How might we improve this step?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = text.split(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBcNmZJzSTY8"
      },
      "source": [
        "# **Topic Modeling**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6vwelqnTL-"
      },
      "source": [
        "## Training\n",
        "\n",
        "We start by instantiating BERTopic. We set language to `english` since our documents are in the English language.\n",
        "\n",
        "We will also calculate the topic probabilities which will be useful later.\n",
        "\n",
        "**NB:** On a 4 CPU UCloud instance, this takes around 13 minutes to run!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfhfzqkoSJ1I"
      },
      "outputs": [],
      "source": [
        "# initialize the model\n",
        "topic_model = BERTopic(language=\"english\", \n",
        "                       calculate_probabilities=True, \n",
        "                       verbose=True)\n",
        "\n",
        "# notice how we use the same fit_transform logic as we've seen before\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJs94tXDo4f6"
      },
      "source": [
        "**NOTE**: Use `language=\"multilingual\"` to select a model that support 50+ languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5O3KpHTnVpz"
      },
      "source": [
        "## Extracting Topics\n",
        "After fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ScBUgXn06IK6",
        "outputId": "ee49a346-8d12-41c4-c7e9-8c1b6782c636"
      },
      "outputs": [],
      "source": [
        "freq = topic_model.get_topic_info()\n",
        "freq.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BtOgifV7Q-H"
      },
      "source": [
        "-1 refers to all outliers and should typically be ignored. Next, let's take a look at a frequent topic that were generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVpvT4bA6KiN",
        "outputId": "9cf99b89-30bb-45fe-b98b-063f8f3624d9"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic(2)  # Select the most frequent topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Get top 10 topics__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCMHaWVMpbo3"
      },
      "outputs": [],
      "source": [
        "topic_model.topics_[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbT2Bd9gqaJ3"
      },
      "source": [
        "# **Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8c8LenB8Zyl"
      },
      "source": [
        "## Visualize Topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "S9qDqEHddgKq",
        "outputId": "3fddd5f1-194e-4708-a7dc-f0c5602c140a"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITB7bf6q8nWQ"
      },
      "source": [
        "## Visualize Topic Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ypfI1-KHdmcX",
        "outputId": "4fd56127-4143-4cf7-d227-ef43725f1ad7"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_distribution(probs[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHRTeSpl5JYB"
      },
      "source": [
        "## Visualize Topic Hierarchy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "ltmLFRR56a4X",
        "outputId": "0458e59c-30ce-4700-8dbf-0932afc63f1d"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_hierarchy(top_n_topics=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4spXl2_C6flq"
      },
      "source": [
        "## Visualize Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "zpm9LsKW6mi5",
        "outputId": "1197affc-dde2-44c1-9ba7-c9fb36a1143c"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart(top_n_topics=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCPdi6_z6sbT"
      },
      "source": [
        "## Visualize Topic Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "edzNhZuZ6wTr",
        "outputId": "e01231db-fe82-49d3-a96f-8135522d9b9b"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_heatmap(n_clusters=20, \n",
        "                              width=1000, \n",
        "                              height=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Search Topics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "similar_topics, similarity = topic_model.find_topics(\"love\", top_n=5)\n",
        "print(similar_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_topic(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.get_representative_docs(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D48ienfZrfP0"
      },
      "source": [
        "# **Topic Representation**\n",
        "After having created the topic model, you might not be satisfied with some of the parameters you have chosen. Fortunately, BERTopic allows you to update the topics after they have been created. \n",
        "\n",
        "This allows for fine-tuning the model to your specifications and wishes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4m3UMsw-Zxk"
      },
      "source": [
        "## Update Topics\n",
        "\n",
        "One of the easiest ways to make our topics more coherent is to revise the *topic descriptions* rather than training a new model. \n",
        "\n",
        "This means that we try to foreground more potentially significant and meaningful words for individual topics.\n",
        "\n",
        "We can do this by creating a simple bag-of-words model and filtering based on some conditions - just like we do when working with scikit-learn. \n",
        "\n",
        "**NB:** This will also take around 25 mins to run on a 4 CPU machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWm7B-FJ-iYW"
      },
      "outputs": [],
      "source": [
        "vectorizer_model = CountVectorizer(stop_words=\"english\", \n",
        "                                   ngram_range=(1, 2),\n",
        "                                   min_df=0.05,\n",
        "                                   max_df=0.9)\n",
        "topic_model.update_topics(docs, vectorizer_model=vectorizer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf31gQavdtfG",
        "outputId": "b2cf3d47-f665-44fe-bbe0-c927e4d73363"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic(1)   # We select topic that we viewed before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic topic models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first need to create some kind of *timestamps* for our data.\n",
        "\n",
        "**Question:** In this context, what might be a good idea for creating timestamps? What is this cell below doing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timestamps = list(range(len(docs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics_over_time = topic_model.topics_over_time(docs, \n",
        "                                                timestamps, \n",
        "                                                nr_bins=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Plotting topics over time__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.visualize_topics_over_time(topics_over_time, \n",
        "                                       normalize_frequency=True, \n",
        "                                       top_n_topics=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Plot specific topics__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_model.visualize_topics_over_time(topics_over_time, \n",
        "                                       normalize_frequency=True, \n",
        "                                       topics=[5]) # list of chosen topics"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
