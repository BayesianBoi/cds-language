# Assignment 5 - Evaluating environmental impact of your exam portfolio

## Repository Overview
This repository contains the analysis of the environmental impact of code used in previous assignments in the Language Analytics course. We use CodeCarbon to measure CO₂ emissions generated by the scripts and present the results in terms of CO₂-equivalent (CO₂eq) emissions.

### Assignment Objective
The objective of this assignment is to find:
- Which assignment generated the most emissions in terms of CO₂eq? Explain why this might be.
- Which specific tasks generated the most emissions in terms of CO₂eq? Again, explain why this might be.
- How robust do you think these results are and how/where might they be improved?
  
## Steps for running the analysis

### Running the Analysis
1. **Install the requirements:**
    ```bash
    bash setup.sh
    ```
2. **Run the notebook for analysis and visualization:**
    ```bash
    jupyter notebook analysis.ipynb
    ```

### Summary of the results
The results of the analysis are presented in the notebook found in the `in` folder. Below is a summary of key findings:

**Emissions from the assignments**
- **Assignment 1:** 2.19g CO₂eq
- **Assignment 2:** 0.45g CO₂eq
- **Assignment 3:** 0.03g CO₂eq
- **Assignment 4:** 26.56g CO₂eq

**Emissions from the scripts**
- **Assignment 1: nlp_analysis.py:** 2.18 g CO2(eq)
- **Assignment 2: vectorize_data.py:** 0.15 g CO2(eq)
- **Assignment 2: neural_network.py:** 0.29 g CO2(eq)
- **Assignment 2: logistic_regression.py:** 0.00 g CO2(eq)
- **Assignment 3: lyrics_query:** 0.03 g CO2(eq)
- **Assignment 4: process_data.py:** 26.54 g CO2(eq)
- **Assignment 4: plot_data.py:** 0.01 g CO2(eq)
- **Assignment 4: top_character.py:** 0.00 g CO2(eq)

**Total emissions for all assignments combined**
*Total:* 29.23g CO₂eq

### Plots of the emissions generated by the assignments:
![Assignment](https://github.com/BayesianBoi/cds-language/blob/main/assignments/assignment%205/out/emissions_by_assignment.png)


### Plot of the emissions generated by the individual scripts:

![Scripts](https://github.com/BayesianBoi/cds-language/blob/main/assignments/assignment%205/out/emissions_by_script.png)


## Interpretation of results
### Assignment with the most emissions:
- Assignment 4 generated the most emissions, which was pretty much only from the process_data.py script which also had the highest CO₂ emissions among all of the scripts. This script handled the emotion classification of the GoT data set.
### Reasons for the high emissions caused by process_data script:
- The dataset contains many lines of dialogue, each of which needs to be processed. I even split up the processing into batches of 10 to optimise the processing but it still had to go through around 2.5k batches- meaning that the data set contains around 25k lines of dialog that the model has to process.

- Emotion Classification: The emotional classification uses a transformers pipeline, which is based on deep learning. These models takes a lot of computational resources to process each sentence. Furthermore, the classification had to identify multiple different emotions and as I wanted to see which character showed the most of an emotion during each season, the classification had to append the emotional scores for each line to the original data set, which upped the computational load also.

## Implications
The total emissions caused by running all of the portfolios is 29.23g of CO2 equavilent. This is equal to brewing 3/4 of a cup of [coffee](https://www.co2everything.com/co2e-of/coffee). While this might seem low, it is important to remember that the emissions were only tracked for the final run of the portfolios and not all of the testing and faulty runs that my scripts had. Also, it is important to note that purely computionally then the language portfolio is far less heavy than the visual was, which we did not track the emissions of.

## Reliability of the CarbonCode tracking
While I believe the tool can act as a good baseline for estimating the emissions caused by the code, it would be impossible to precisely track the CO2 emission caused by running code. If they truly were to capture the CO_2 emissions of the code, they would need to have access to real-time updates on how much of the energy on the power grid is renewable at that time for the specific country (and even region) where the setup is located. That would be a major undertaking, so I'd assume they use a flat baseline for each country (or region) so that CO2 emissions are calculated based on an average day. Another aspect is that the tool only seems to capture how much power the hardware is using and calculating the emissions solely based on that. However, it might not incorporate the hardware production cost (in terms of cost of CO2). For instance, the UCloud system that we have been using for the portfolios uses a hardware setup that was purely setup for doing tasks like we have been doing. Therefore, it serves no second purpose, and the CO2 emissions of producing and setting up the system should also be included in the calculation. This would also be an impossible undertaking (what is the expected lifetime of the system, what are the benefits of using it, is it used for different purposes and how do we calculate a CO2 cap for that).

While the tool might not be precise down to the wire, the tool can still act as a baseline and more importantly as a powerful awareness tool of just how much power the increasingly growing AI-industry needs to function.
