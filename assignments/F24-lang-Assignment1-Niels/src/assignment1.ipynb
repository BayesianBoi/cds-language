{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Analyzing Swedish high school English corpus across different grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_237/1379384044.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Loading necessary packages\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/ucloud/.local/lib/python3.10/site-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ucloud/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ucloud/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /home/ucloud/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ucloud/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ucloud/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucloud/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucloud/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucloud/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ucloud/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ucloud/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/ucloud/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ucloud/.local/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# downloading medium-sized spaCy NLP model\n",
    "!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the \"USEcorpus\" folder\n",
    "main_folder = (\"..\" \"/in/USEcorpus\")\n",
    "\n",
    "# list of subfolders in the USEcorpus. Also sorting.\n",
    "subfolders = sorted(os.listdir(main_folder))\n",
    "\n",
    "# load spacy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# output folder\n",
    "output_folder = (\"..\" \"/out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\" \n",
    "    Clean-up function that removes punctionation, \\n, \\t and characters inbetween <>, and lowercases the string.\n",
    "    \n",
    "    Args:\n",
    "    - text (string): A string containing a text file\n",
    "\n",
    "    Returns:\n",
    "    - A text (string): Returns the same string but pre-processed\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercasing the text\n",
    "    text = re.sub(r\"<[^>]*>\", \"\", text) # remove metadata text contained within \"<>\"\n",
    "    text = re.sub(r\"\\n\", \"\", text) #removing \\n, which occured often in the text files\n",
    "    text = re.sub(r\"\\t\", \"\", text) #removing \\t, which occured often in the text files\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~''' # list of punctuations\n",
    "\n",
    "    for char in text: #removing the defined punctuations from the text\n",
    "        if char in punc:\n",
    "            text = text.replace(char, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nlp_metrics(texts):\n",
    "    \"\"\"\n",
    "    Calculate NLP metrics for a list of texts.\n",
    "    \n",
    "    Args:\n",
    "    - texts (list): A list of strings representing text documents.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    nlp_metrics = [] #empty list for the metrics\n",
    "\n",
    "    for i, text in enumerate(texts, 1):\n",
    "        # convert the individual text into a doc (using the medium-sized model)\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Calculate various NLP metrics\n",
    "        num_words = len(doc) # calculate the number of words in the text file\n",
    "        num_tokens = doc.count_by(spacy.attrs.POS) # calculate the number of tokens in the text file\n",
    "        rel_freq_noun = round(num_tokens.get(spacy.parts_of_speech.NOUN, 0) / num_words * 10000, 1) # relative number of nouns in text file\n",
    "        rel_freq_verb = round(num_tokens.get(spacy.parts_of_speech.VERB, 0) / num_words * 10000, 1) # relative number of verbs in text file\n",
    "        rel_freq_adj = round(num_tokens.get(spacy.parts_of_speech.ADJ, 0) / num_words * 10000, 1) # relative number of adjectives in text file\n",
    "        rel_freq_adv = round(num_tokens.get(spacy.parts_of_speech.ADV, 0) / num_words * 10000, 1) # relative number of adverbs in text file\n",
    "\n",
    "        # using the ent (EntityRecognizer) from SpaCy to identify unique tokens (or span of tokens). This is then used to identify the number of unique entities in regards to persons, locations and organizations.\n",
    "        unique_entities = set([(ent.text, ent.label_) for ent in doc.ents]) \n",
    "\n",
    "        # counting the number of unique entities in the text in regards to persons, locataions and organizations\n",
    "        unique_per = sum(1 for ent in unique_entities if ent[1] == \"PERSON\")\n",
    "        unique_loc = sum(1 for ent in unique_entities if ent[1] == \"LOC\")\n",
    "        unique_org = sum(1 for ent in unique_entities if ent[1] == \"ORG\")\n",
    "\n",
    "        # append the metrics to nlp_metrics\n",
    "        nlp_metrics.append([f\"file{i}.txt\", rel_freq_noun, rel_freq_verb, rel_freq_adj, rel_freq_adv, unique_per, unique_loc, unique_org])\n",
    "\n",
    "    # Convert nlp_metrics to DataFrame\n",
    "    columns = [\"Filename\", \"RelFreq NOUN\", \"RelFreq VERB\", \"RelFreq ADJ\", \"RelFreq ADV\", \"Unique PER\", \"Unique LOC\", \"Unique ORG\"]\n",
    "    return pd.DataFrame(nlp_metrics, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text files in a1...\n",
      "Processing text files in a2...\n",
      "Processing text files in a3...\n",
      "Processing text files in a4...\n",
      "Processing text files in a5...\n",
      "Processing text files in b1...\n",
      "Processing text files in b2...\n",
      "Processing text files in b3...\n",
      "Processing text files in b4...\n",
      "Processing text files in b5...\n",
      "Processing text files in b6...\n",
      "Processing text files in b7...\n",
      "Processing text files in b8...\n",
      "Processing text files in c1...\n"
     ]
    }
   ],
   "source": [
    "# making a dictionary to store the texts from each subfolder\n",
    "subfolder_texts = {}\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    # getting the path of the specific subfolder\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "\n",
    "    print(f\"Loading and pre-processing text files in {subfolder}...\")\n",
    "    # making a list to store the text from the files in the subfolder\n",
    "    texts = []\n",
    "\n",
    "    # open each individual text file in the specific sorted subfolder and stores these as one long string for each subfolder\n",
    "    for file in sorted(os.listdir(subfolder_path)):\n",
    "        file_path = os.path.join(subfolder_path, file)\n",
    "        # using ISO-8859-1 which ensures that all of the files can be loaded (some of them contain characters which otherwise cannot be loaded)\n",
    "        with open(file_path, \"r\", encoding = \"ISO-8859-1\") as file:\n",
    "            # reading the contents of the text files\n",
    "            text = file.read()\n",
    "            # pre-processes the text\n",
    "            text = preprocess_text(text)\n",
    "            texts.append(text)\n",
    "    # storing the texts from each folder in a dictionary\n",
    "    subfolder_texts[subfolder] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing analysis for a1...\n",
      "Performing analysis for a2...\n",
      "Performing analysis for a3...\n",
      "Performing analysis for a4...\n",
      "Performing analysis for a5...\n",
      "Performing analysis for b1...\n",
      "Performing analysis for b2...\n",
      "Performing analysis for b3...\n",
      "Performing analysis for b4...\n",
      "Performing analysis for b5...\n",
      "Performing analysis for b6...\n",
      "Performing analysis for b7...\n",
      "Performing analysis for b8...\n",
      "Performing analysis for c1...\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary to store metrics for each subfolder\n",
    "subfolder_metrics = {}\n",
    "\n",
    "# for loop that calculates the NLP metrics for an individual folder at a time and stores it\n",
    "for subfolder, texts in subfolder_texts.items():\n",
    "    print(f\"Performing analysis for {subfolder}...\")\n",
    "    \n",
    "    # calculate NLP metrics for the current subfolder's texts\n",
    "    subfolder_df = calculate_nlp_metrics(texts)\n",
    "    \n",
    "    # Store the dataframe in the specified dictionary\n",
    "    subfolder_metrics[subfolder] = subfolder_df\n",
    "\n",
    "\n",
    "# Create CSV files for each subfolder in the output folder\n",
    "for subfolder, df in subfolder_metrics.items():\n",
    "    df.to_csv(os.path.join(output_folder, f\"nlp_analysis_results_{subfolder}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
